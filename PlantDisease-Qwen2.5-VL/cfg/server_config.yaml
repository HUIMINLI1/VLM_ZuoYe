# llama-server环境变量-------------------------------------------------------------------------------------------------------
SERVER: E:\Program Files\llama.cpp\build\bin\Release\llama-server.exe # llama-server位置
PORT: 8080
MODEL_PATH: G:\huggingface\Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-f16.gguf # Qwen主模型位置
# Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-f16.gguf
MMPROJ_PATH: G:\huggingface\Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-mmproj-bf16.gguf # 视觉编码器位置
# Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-mmproj-bf16.gguf

# llama-server模型参数 https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md
# 警告：更改调试好的参数前建议备份
gpu-layers: -1
keep: 128
ctx-size: 32768
temperature: 0.1
top-k: 4
top-p: 0.8
repeat-penalty: 1.1
